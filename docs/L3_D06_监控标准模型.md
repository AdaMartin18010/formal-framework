---
id: L3_D06_STD_V1.0
title: 监控标准模型（L3）
level: L3
domain: D06
version: V1.0
status: enhanced
author: Formal Framework Team
date: 2024-12-19
tags: [monitoring, observability, metrics, alerting, tracing]
---

**本节要点**：（1）指标、告警、日志、追踪、仪表盘标准化结构；（2）与 L2_D06 及 OpenTelemetry/Prometheus 等标准的对齐；（3）不变式（如 SLOCoverage、AlertConsistency）；（4）行业映射见 L4_D90 可观测性、金融审计、IoT 设备监控。  
**预计阅读时间**：全文约 20–30 分钟；仅读 §1–§2 约 8 分钟。  
**单次阅读建议**：若一次读完超过 40 分钟，建议分 2–3 次阅读，每次 1–2 节；结合 [REVIEW_CHECKLIST](learning/REVIEW_CHECKLIST.md) 做阶段自测。

## 1. 概述与关联

### 1.1 前置与关联

- **对应 L2 元模型**：[L2_D06 监控元模型](L2_D06_监控元模型.md)；对象/属性/不变式映射详情见 [L2↔L3 映射总表 2.5 节（监控）](formal-model/alignment-L2-L3-matrix.md#25-监控d06)。
- **对应理论**：[formal-model/monitoring-model/theory.md](formal-model/monitoring-model/theory.md)
- **与权威对标**：本 L3 与标准/课程知识点对照见 [AUTHORITY_STANDARD_COURSE_L2L3_MATRIX](reference/AUTHORITY_STANDARD_COURSE_L2L3_MATRIX.md)、[AUTHORITY_ALIGNMENT_INDEX](reference/AUTHORITY_ALIGNMENT_INDEX.md) 第 2–4 节。

### 1.2 范围

- 指标、告警、日志、追踪、仪表盘

**本段检查点**：此处可暂停；建议先能说出 L2_D06 与 L3_D06 的对应关系及五类可观测性要素（指标、告警、日志、追踪、仪表盘），再继续 §2。自测可参考 [概念索引·监控模型](knowledge-standards/concept-index/CONCEPT_INDEX.md#监控模型-monitoring-model) 与 [REVIEW_CHECKLIST](learning/REVIEW_CHECKLIST.md)。

## 2. 核心结构

```yaml
ObservabilityStandard:
  Metric:
    formal_spec: |
      Metric = {
        name: String
        kind: {counter, gauge, histogram, summary}
        labels: Set<Label>
        unit: Unit
        retention: Duration
      }

  Alert:
    formal_spec: |
      Alert = {
        expr: PromQL
        severity: {info, warning, critical}
        for: Duration
        annotations: {summary, description}
        runbook: URL
      }

  Trace:
    formal_spec: |
      Trace = {
        service: ServiceName
        span: Span
        attributes: KeyValueMap
        latency_buckets: Histogram
      }

  Dashboard:
    formal_spec: |
      Dashboard = {
        panels: Seq<Panel>
        variables: Set<Variable>
        links: Set<Link>
      }
```

## 3. 形式化规范

```text
// SLI/SLO 覆盖
Invariant MON1 (SLOCoverage):
  ∀ svc. defined(SLI(svc), SLO(svc)) ⇒ exists(Alert for SLO violation)

// 标签基数上限
Invariant MON2 (LabelCardinalityBound):
  ∀ m ∈ Metrics. |cardinality(m.labels)| ≤ bound(m)

// Runbook 存在
Invariant MON3 (RunbookPresence):
  ∀ a ∈ Alerts. exists(a.runbook)

// 指标命名规范
Invariant MON4 (MetricNaming):
  ∀ m. name(m) matches ^[a-z_][a-z0-9_]*(_seconds|_bytes|_total)?$
```

## 4. 监控架构与实现

### 4.1 三层监控架构

```yaml
monitoring_architecture:
  collection_layer:
    agents:
      - name: "node_exporter"
        type: "system_metrics"
        targets: ["all_nodes"]
        
      - name: "prometheus"
        type: "metrics_collector"
        scrape_interval: "15s"
        
      - name: "jaeger_agent"
        type: "trace_collector"
        sampling_rate: 0.1
        
    collectors:
      - name: "fluentd"
        type: "log_collector"
        sources: ["containers", "system", "applications"]
        
  storage_layer:
    metrics_storage:
      - name: "prometheus"
        retention: "30d"
        compression: true
        
    logs_storage:
      - name: "elasticsearch"
        retention: "7d"
        shards: 3
        
    traces_storage:
      - name: "jaeger"
        retention: "3d"
        sampling: "adaptive"
        
  presentation_layer:
    dashboards:
      - name: "grafana"
        datasources: ["prometheus", "elasticsearch", "jaeger"]
        
    alerting:
      - name: "alertmanager"
        routing: "hierarchical"
        grouping: "by_service"
```

### 4.2 SLI/SLO 定义

```yaml
sli_slo_definitions:
  availability:
    sli: "uptime_percentage"
    measurement: "successful_requests / total_requests"
    target: 99.9
    window: "30d"
    
  latency:
    sli: "response_time_p95"
    measurement: "95th_percentile(response_time)"
    target: 200
    unit: "ms"
    window: "5m"
    
  error_rate:
    sli: "error_percentage"
    measurement: "error_requests / total_requests"
    target: 0.1
    unit: "percent"
    window: "5m"
    
  throughput:
    sli: "requests_per_second"
    measurement: "total_requests / time_window"
    target: 1000
    unit: "rps"
    window: "1m"
```

### 4.3 告警规则与路由

```yaml
alerting_rules:
  critical_alerts:
    - name: "service_down"
      condition: "up == 0"
      duration: "1m"
      severity: "critical"
      summary: "Service {{ $labels.job }} is down"
      
    - name: "high_error_rate"
      condition: "rate(http_requests_total{status=~'5..'}[5m]) > 0.1"
      duration: "2m"
      severity: "critical"
      summary: "High error rate on {{ $labels.service }}"
      
  warning_alerts:
    - name: "high_latency"
      condition: "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5"
      duration: "5m"
      severity: "warning"
      summary: "High latency on {{ $labels.service }}"
      
    - name: "high_memory_usage"
      condition: "container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.8"
      duration: "10m"
      severity: "warning"
      summary: "High memory usage on {{ $labels.pod }}"

alert_routing:
  routes:
    - match:
        severity: "critical"
      receiver: "oncall-team"
      group_wait: "0s"
      group_interval: "5m"
      
    - match:
        severity: "warning"
      receiver: "dev-team"
      group_wait: "30s"
      group_interval: "15m"
      
  receivers:
    - name: "oncall-team"
      slack_configs:
        - channel: "#alerts-critical"
          title: "Critical Alert"
          
    - name: "dev-team"
      email_configs:
        - to: "dev-team@company.com"
          subject: "Warning Alert"
```

## 5. 可观测性实现

### 5.1 指标收集 (Metrics)

```yaml
metrics_collection:
  application_metrics:
    - name: "http_requests_total"
      type: "counter"
      labels: ["method", "endpoint", "status"]
      description: "Total HTTP requests"
      
    - name: "http_request_duration_seconds"
      type: "histogram"
      buckets: [0.1, 0.5, 1.0, 2.0, 5.0]
      labels: ["method", "endpoint"]
      description: "HTTP request duration"
      
    - name: "active_connections"
      type: "gauge"
      labels: ["service"]
      description: "Active database connections"
      
  system_metrics:
    - name: "cpu_usage_percent"
      type: "gauge"
      labels: ["instance", "cpu"]
      description: "CPU usage percentage"
      
    - name: "memory_usage_bytes"
      type: "gauge"
      labels: ["instance"]
      description: "Memory usage in bytes"
      
    - name: "disk_io_operations_total"
      type: "counter"
      labels: ["instance", "device", "operation"]
      description: "Disk I/O operations"
```

### 5.2 日志收集 (Logs)

```yaml
log_collection:
  log_sources:
    - name: "application_logs"
      source: "containers"
      format: "json"
      fields:
        - "timestamp"
        - "level"
        - "message"
        - "service"
        - "trace_id"
        
    - name: "access_logs"
      source: "nginx"
      format: "combined"
      fields:
        - "remote_addr"
        - "time_local"
        - "request"
        - "status"
        - "body_bytes_sent"
        
    - name: "system_logs"
      source: "journald"
      format: "systemd"
      fields:
        - "timestamp"
        - "hostname"
        - "unit"
        - "message"
        
  log_processing:
    parsing:
      - name: "json_parser"
        type: "json"
        fields: ["timestamp", "level", "message"]
        
      - name: "regex_parser"
        type: "regex"
        pattern: '(\S+) - - \[([^\]]+)\] "([^"]+)" (\d+) (\d+)'
        fields: ["remote_addr", "time_local", "request", "status", "body_bytes_sent"]
        
    enrichment:
      - name: "geoip"
        source: "remote_addr"
        target: "geoip"
        
      - name: "user_agent"
        source: "user_agent"
        target: "browser_info"
```

### 5.3 链路追踪 (Traces)

```yaml
trace_collection:
  instrumentation:
    - name: "http_server"
      type: "auto"
      libraries: ["opentelemetry-http"]
      
    - name: "database_client"
      type: "auto"
      libraries: ["opentelemetry-db"]
      
    - name: "message_queue"
      type: "manual"
      libraries: ["opentelemetry-kafka"]
      
  sampling_strategies:
    - name: "head_based"
      type: "probabilistic"
      rate: 0.1
      
    - name: "tail_based"
      type: "adaptive"
      min_rate: 0.01
      max_rate: 0.5
      
  trace_attributes:
    - name: "service.name"
      value: "user-service"
      
    - name: "service.version"
      value: "1.2.3"
      
    - name: "deployment.environment"
      value: "production"
```

## 6. 仪表盘与可视化

### 6.1 服务仪表盘

```yaml
service_dashboard:
  title: "User Service Dashboard"
  panels:
    - title: "Request Rate"
      type: "graph"
      targets:
        - expr: "rate(http_requests_total[5m])"
          legend: "{{method}} {{endpoint}}"
      y_axes:
        - unit: "reqps"
          
    - title: "Response Time"
      type: "graph"
      targets:
        - expr: "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          legend: "95th percentile"
        - expr: "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))"
          legend: "50th percentile"
      y_axes:
        - unit: "s"
          
    - title: "Error Rate"
      type: "graph"
      targets:
        - expr: "rate(http_requests_total{status=~'5..'}[5m]) / rate(http_requests_total[5m])"
          legend: "Error rate"
      y_axes:
        - unit: "percent"
          
    - title: "Active Connections"
      type: "stat"
      targets:
        - expr: "active_connections"
      thresholds:
        - color: "green"
          value: 0
        - color: "yellow"
          value: 50
        - color: "red"
          value: 100
```

### 6.2 基础设施仪表盘

```yaml
infrastructure_dashboard:
  title: "Infrastructure Overview"
  panels:
    - title: "CPU Usage"
      type: "graph"
      targets:
        - expr: "100 - (avg(rate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)"
          legend: "CPU Usage %"
      y_axes:
        - unit: "percent"
        - min: 0
        - max: 100
        
    - title: "Memory Usage"
      type: "graph"
      targets:
        - expr: "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100"
          legend: "Memory Usage %"
      y_axes:
        - unit: "percent"
        - min: 0
        - max: 100
        
    - title: "Disk Usage"
      type: "graph"
      targets:
        - expr: "(1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100"
          legend: "Disk Usage %"
      y_axes:
        - unit: "percent"
        - min: 0
        - max: 100
        
    - title: "Network I/O"
      type: "graph"
      targets:
        - expr: "rate(node_network_receive_bytes_total[5m])"
          legend: "Receive"
        - expr: "rate(node_network_transmit_bytes_total[5m])"
          legend: "Transmit"
      y_axes:
        - unit: "bytes"
```

## 7. 与 L2/L4 映射

### 7.1 L2 元模型映射

- **L2_D06_监控元模型**：指标、日志、追踪、告警语义对齐
- **L2_D04_运行时元模型**：容器、网络、编排、存储监控依赖
- **L2_D10_分布式模式**：服务发现、负载均衡、容错模式监控

### 7.2 L4 行业对齐

- **云原生对标**：Prometheus/Alertmanager/Grafana、OpenTelemetry/Tempo/Jaeger
- **金融行业**：实时交易监控、合规审计、风险指标
- **IoT 行业**：设备状态监控、边缘计算、数据采集
- **AI 基础设施**：模型性能监控、训练指标、推理延迟

### 7.3 引用本模型的行业案例

- [云原生案例一：Kubernetes（健康检查/资源监控）](industry-model/cloud-native-architecture/README.md#案例一kubernetes-集群编排基础)
- [云原生案例二：Service Mesh（可观测性）](industry-model/cloud-native-architecture/README.md#案例二service-mesh-流量治理istio)
- [云原生案例四：可观测性一体化（Prometheus+OTel）](industry-model/cloud-native-architecture/README.md#案例四可观测性一体化prometheusotel)
- [云原生案例六：API 网关（监控/追踪）](industry-model/cloud-native-architecture/README.md#案例六api-网关流量治理kongenvoy)
- 金融、IoT、AI、Web3 — 见各 [industry-model](industry-model/) README。

## 8. 权威对标

- **标准与证据**：本模型与 ISO/IEC 25010 质量模型、L2_D06 监控元模型对应。参见 [权威对标总索引](reference/AUTHORITY_ALIGNMENT_INDEX.md) 第 2 节；CNCF PCA、OTCA 见第 4 节。
- **课程映射**：监控/可观测性对应 LEARNING_PATHS 云原生专项；认证见 AUTHORITY_ALIGNMENT_INDEX 第 4 节。

参见：[L2↔L3 映射总表](formal-model/alignment-L2-L3-matrix.md)
