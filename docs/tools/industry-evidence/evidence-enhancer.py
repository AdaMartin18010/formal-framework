#!/usr/bin/env python3
"""
è¡Œä¸šè¯æ®æ¡ç›®å®è¯åŒ–å¢å¼ºç³»ç»Ÿ
è‡ªåŠ¨æ”¶é›†ã€éªŒè¯å’Œé“¾æ¥å¤–éƒ¨è¯æ®æ¥æº
"""

import json
import requests
import hashlib
import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import re
from urllib.parse import urlparse
import time


class EvidenceType(Enum):
    """è¯æ®ç±»å‹"""
    ACADEMIC_PAPER = "academic_paper"
    INDUSTRY_REPORT = "industry_report"
    STANDARD_DOCUMENT = "standard_document"
    CASE_STUDY = "case_study"
    TECHNICAL_DOCUMENTATION = "technical_documentation"
    BENCHMARK_RESULT = "benchmark_result"
    CERTIFICATION = "certification"
    WHITE_PAPER = "white_paper"


class EvidenceSource(Enum):
    """è¯æ®æ¥æº"""
    IEEE = "ieee"
    ACM = "acm"
    NIST = "nist"
    ISO = "iso"
    INDUSTRY = "industry"
    GITHUB = "github"
    RESEARCH_GATE = "research_gate"
    ARXIV = "arxiv"
    GOOGLE_SCHOLAR = "google_scholar"


@dataclass
class EvidenceEntry:
    """è¯æ®æ¡ç›®"""
    evidence_id: str
    title: str
    evidence_type: EvidenceType
    source: EvidenceSource
    url: str
    doi: Optional[str]
    authors: List[str]
    publication_date: datetime.datetime
    abstract: str
    keywords: List[str]
    relevance_score: float
    verification_status: str
    metadata: Dict[str, Any]


@dataclass
class IndustryCapability:
    """è¡Œä¸šèƒ½åŠ›"""
    capability_id: str
    industry: str
    domain: str
    capability_name: str
    description: str
    required_evidence_count: int
    current_evidence_count: int
    evidence_entries: List[str]
    coverage_score: float


class EvidenceEnhancer:
    """è¯æ®å¢å¼ºç³»ç»Ÿ"""
    
    def __init__(self, config_path: str = "evidence-config.json"):
        self.config = self.load_config(config_path)
        self.evidence_entries: Dict[str, EvidenceEntry] = {}
        self.industry_capabilities: Dict[str, IndustryCapability] = {}
        self.search_apis = self._initialize_search_apis()
    
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "search_apis": {
                "ieee": {
                    "base_url": "https://ieeexploreapi.ieee.org/api/v1/search/articles",
                    "api_key": "your_ieee_api_key",
                    "enabled": True
                },
                "arxiv": {
                    "base_url": "http://export.arxiv.org/api/query",
                    "enabled": True
                },
                "crossref": {
                    "base_url": "https://api.crossref.org/works",
                    "enabled": True
                },
                "github": {
                    "base_url": "https://api.github.com/search/repositories",
                    "token": "your_github_token",
                    "enabled": True
                }
            },
            "industries": {
                "cloud_native": {
                    "domains": ["kubernetes", "microservices", "containerization", "devops"],
                    "required_evidence_per_domain": 3,
                    "priority_keywords": ["kubernetes", "docker", "istio", "prometheus", "grafana"]
                },
                "finance": {
                    "domains": ["open_banking", "payment_systems", "risk_management", "compliance"],
                    "required_evidence_per_domain": 3,
                    "priority_keywords": ["open_banking", "pci_dss", "iso_20022", "fraud_detection"]
                },
                "iot": {
                    "domains": ["mqtt", "device_management", "edge_computing", "security"],
                    "required_evidence_per_domain": 3,
                    "priority_keywords": ["mqtt", "coap", "edge_computing", "iot_security"]
                },
                "ai_infrastructure": {
                    "domains": ["mlops", "model_serving", "data_pipelines", "model_governance"],
                    "required_evidence_per_domain": 3,
                    "priority_keywords": ["tensorflow", "pytorch", "mlops", "model_serving"]
                },
                "web3": {
                    "domains": ["blockchain", "smart_contracts", "defi", "nft"],
                    "required_evidence_per_domain": 3,
                    "priority_keywords": ["ethereum", "solidity", "defi", "smart_contracts"]
                }
            },
            "quality_thresholds": {
                "min_relevance_score": 0.7,
                "min_publication_year": 2018,
                "max_publication_year": 2024,
                "min_abstract_length": 100,
                "required_verification_sources": 2
            }
        }
    
    def _initialize_search_apis(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æœç´¢API"""
        return {
            "ieee": self._create_ieee_searcher(),
            "arxiv": self._create_arxiv_searcher(),
            "crossref": self._create_crossref_searcher(),
            "github": self._create_github_searcher()
        }
    
    def _create_ieee_searcher(self):
        """åˆ›å»ºIEEEæœç´¢å™¨"""
        def search_ieee(query: str, max_results: int = 10) -> List[Dict[str, Any]]:
            try:
                api_key = self.config["search_apis"]["ieee"]["api_key"]
                base_url = self.config["search_apis"]["ieee"]["base_url"]
                
                params = {
                    "apikey": api_key,
                    "format": "json",
                    "max_records": max_results,
                    "querytext": query,
                    "sort_field": "relevance",
                    "sort_order": "desc"
                }
                
                response = requests.get(base_url, params=params, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    return data.get("articles", [])
                else:
                    print(f"IEEE API error: {response.status_code}")
                    return []
            except Exception as e:
                print(f"IEEE search error: {str(e)}")
                return []
        
        return search_ieee
    
    def _create_arxiv_searcher(self):
        """åˆ›å»ºArXivæœç´¢å™¨"""
        def search_arxiv(query: str, max_results: int = 10) -> List[Dict[str, Any]]:
            try:
                base_url = self.config["search_apis"]["arxiv"]["base_url"]
                
                params = {
                    "search_query": f"all:{query}",
                    "start": 0,
                    "max_results": max_results,
                    "sortBy": "relevance",
                    "sortOrder": "descending"
                }
                
                response = requests.get(base_url, params=params, timeout=30)
                if response.status_code == 200:
                    # è§£æArXiv XMLå“åº”
                    import xml.etree.ElementTree as ET
                    root = ET.fromstring(response.content)
                    
                    entries = []
                    for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
                        entry_data = {
                            "title": entry.find("{http://www.w3.org/2005/Atom}title").text,
                            "summary": entry.find("{http://www.w3.org/2005/Atom}summary").text,
                            "published": entry.find("{http://www.w3.org/2005/Atom}published").text,
                            "id": entry.find("{http://www.w3.org/2005/Atom}id").text,
                            "authors": [author.find("{http://www.w3.org/2005/Atom}name").text 
                                      for author in entry.findall("{http://www.w3.org/2005/Atom}author")]
                        }
                        entries.append(entry_data)
                    
                    return entries
                else:
                    print(f"ArXiv API error: {response.status_code}")
                    return []
            except Exception as e:
                print(f"ArXiv search error: {str(e)}")
                return []
        
        return search_arxiv
    
    def _create_crossref_searcher(self):
        """åˆ›å»ºCrossRefæœç´¢å™¨"""
        def search_crossref(query: str, max_results: int = 10) -> List[Dict[str, Any]]:
            try:
                base_url = self.config["search_apis"]["crossref"]["base_url"]
                
                params = {
                    "query": query,
                    "rows": max_results,
                    "sort": "relevance",
                    "order": "desc"
                }
                
                response = requests.get(base_url, params=params, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    return data.get("message", {}).get("items", [])
                else:
                    print(f"CrossRef API error: {response.status_code}")
                    return []
            except Exception as e:
                print(f"CrossRef search error: {str(e)}")
                return []
        
        return search_crossref
    
    def _create_github_searcher(self):
        """åˆ›å»ºGitHubæœç´¢å™¨"""
        def search_github(query: str, max_results: int = 10) -> List[Dict[str, Any]]:
            try:
                base_url = self.config["search_apis"]["github"]["base_url"]
                token = self.config["search_apis"]["github"].get("token")
                
                headers = {}
                if token:
                    headers["Authorization"] = f"token {token}"
                
                params = {
                    "q": query,
                    "sort": "stars",
                    "order": "desc",
                    "per_page": max_results
                }
                
                response = requests.get(base_url, params=params, headers=headers, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    return data.get("items", [])
                else:
                    print(f"GitHub API error: {response.status_code}")
                    return []
            except Exception as e:
                print(f"GitHub search error: {str(e)}")
                return []
        
        return search_github
    
    def search_evidence(self, query: str, industry: str, domain: str) -> List[EvidenceEntry]:
        """æœç´¢è¯æ®"""
        print(f"æœç´¢è¯æ®: {query} (è¡Œä¸š: {industry}, é¢†åŸŸ: {domain})")
        
        evidence_entries = []
        
        # ä»å¤šä¸ªæ¥æºæœç´¢
        for source_name, searcher in self.search_apis.items():
            if not self.config["search_apis"][source_name].get("enabled", True):
                continue
            
            try:
                results = searcher(query, max_results=5)
                
                for result in results:
                    evidence_entry = self._parse_search_result(result, source_name, industry, domain)
                    if evidence_entry and self._validate_evidence(evidence_entry):
                        evidence_entries.append(evidence_entry)
                        self.evidence_entries[evidence_entry.evidence_id] = evidence_entry
                
                # é¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                print(f"æœç´¢ {source_name} æ—¶å‡ºé”™: {str(e)}")
        
        return evidence_entries
    
    def _parse_search_result(self, result: Dict[str, Any], source: str, 
                           industry: str, domain: str) -> Optional[EvidenceEntry]:
        """è§£ææœç´¢ç»“æœ"""
        try:
            # ç”Ÿæˆè¯æ®ID
            evidence_id = self._generate_evidence_id(result, source)
            
            # æå–åŸºæœ¬ä¿¡æ¯
            title = result.get("title", "").strip()
            if not title:
                return None
            
            # æå–æ‘˜è¦
            abstract = result.get("summary", result.get("abstract", "")).strip()
            if len(abstract) < self.config["quality_thresholds"]["min_abstract_length"]:
                return None
            
            # æå–ä½œè€…
            authors = []
            if "authors" in result:
                if isinstance(result["authors"], list):
                    authors = [author.get("name", str(author)) for author in result["authors"]]
                else:
                    authors = [str(result["authors"])]
            
            # æå–å‘å¸ƒæ—¥æœŸ
            publication_date = self._parse_publication_date(result)
            if not publication_date:
                return None
            
            # æ£€æŸ¥å¹´ä»½èŒƒå›´
            year = publication_date.year
            min_year = self.config["quality_thresholds"]["min_publication_year"]
            max_year = self.config["quality_thresholds"]["max_publication_year"]
            
            if year < min_year or year > max_year:
                return None
            
            # æå–DOI
            doi = result.get("doi", result.get("DOI"))
            
            # æå–URL
            url = result.get("url", result.get("link", result.get("html_url", "")))
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            relevance_score = self._calculate_relevance_score(result, industry, domain)
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(result, industry, domain)
            
            evidence_entry = EvidenceEntry(
                evidence_id=evidence_id,
                title=title,
                evidence_type=self._determine_evidence_type(result, source),
                source=EvidenceSource(source.upper()) if hasattr(EvidenceSource, source.upper()) else EvidenceSource.INDUSTRY,
                url=url,
                doi=doi,
                authors=authors,
                publication_date=publication_date,
                abstract=abstract,
                keywords=keywords,
                relevance_score=relevance_score,
                verification_status="pending",
                metadata={
                    "industry": industry,
                    "domain": domain,
                    "source_data": result,
                    "search_timestamp": datetime.datetime.now().isoformat()
                }
            )
            
            return evidence_entry
            
        except Exception as e:
            print(f"è§£ææœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _generate_evidence_id(self, result: Dict[str, Any], source: str) -> str:
        """ç”Ÿæˆè¯æ®ID"""
        # ä½¿ç”¨æ ‡é¢˜å’Œæ¥æºç”Ÿæˆå”¯ä¸€ID
        title = result.get("title", "")
        url = result.get("url", result.get("link", ""))
        
        data = f"{source}_{title}_{url}"
        return hashlib.md5(data.encode()).hexdigest()[:16]
    
    def _parse_publication_date(self, result: Dict[str, Any]) -> Optional[datetime.datetime]:
        """è§£æå‘å¸ƒæ—¥æœŸ"""
        date_fields = ["published", "publication_date", "date", "created_at", "updated_at"]
        
        for field in date_fields:
            if field in result:
                try:
                    date_str = result[field]
                    if isinstance(date_str, str):
                        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                        formats = [
                            "%Y-%m-%d",
                            "%Y-%m-%dT%H:%M:%S",
                            "%Y-%m-%dT%H:%M:%SZ",
                            "%Y-%m-%dT%H:%M:%S.%fZ",
                            "%Y-%m-%d %H:%M:%S"
                        ]
                        
                        for fmt in formats:
                            try:
                                return datetime.datetime.strptime(date_str, fmt)
                            except ValueError:
                                continue
                except:
                    continue
        
        return None
    
    def _calculate_relevance_score(self, result: Dict[str, Any], industry: str, domain: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        score = 0.0
        
        # åŸºäºæ ‡é¢˜çš„ç›¸å…³æ€§
        title = result.get("title", "").lower()
        abstract = result.get("summary", result.get("abstract", "")).lower()
        text = f"{title} {abstract}"
        
        # è·å–è¡Œä¸šå…³é”®è¯
        industry_config = self.config["industries"].get(industry, {})
        priority_keywords = industry_config.get("priority_keywords", [])
        domain_keywords = industry_config.get("domains", [])
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        all_keywords = priority_keywords + domain_keywords
        
        for keyword in all_keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in text:
                if keyword in priority_keywords:
                    score += 0.1  # ä¼˜å…ˆçº§å…³é”®è¯æƒé‡æ›´é«˜
                else:
                    score += 0.05  # æ™®é€šå…³é”®è¯æƒé‡
        
        # åŸºäºæ¥æºçš„åˆ†æ•°è°ƒæ•´
        source = result.get("source", "")
        if "ieee" in source.lower():
            score += 0.2
        elif "arxiv" in source.lower():
            score += 0.15
        elif "github" in source.lower():
            score += 0.1
        
        # åŸºäºå¼•ç”¨æ•°çš„åˆ†æ•°è°ƒæ•´ï¼ˆå¦‚æœæœ‰ï¼‰
        citations = result.get("citations", result.get("citation_count", 0))
        if citations > 0:
            score += min(0.2, citations / 100)  # æœ€å¤šå¢åŠ 0.2åˆ†
        
        return min(1.0, score)  # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1.0
    
    def _extract_keywords(self, result: Dict[str, Any], industry: str, domain: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        # ä»æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–å…³é”®è¯
        title = result.get("title", "")
        abstract = result.get("summary", result.get("abstract", ""))
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯ï¼‰
        text = f"{title} {abstract}".lower()
        
        # é¢„å®šä¹‰çš„æŠ€æœ¯å…³é”®è¯
        tech_keywords = [
            "formal verification", "model checking", "theorem proving",
            "kubernetes", "docker", "microservices", "api", "security",
            "blockchain", "smart contracts", "machine learning", "ai",
            "iot", "mqtt", "edge computing", "cloud native"
        ]
        
        for keyword in tech_keywords:
            if keyword in text:
                keywords.append(keyword)
        
        # æ·»åŠ è¡Œä¸šç‰¹å®šå…³é”®è¯
        industry_config = self.config["industries"].get(industry, {})
        domain_keywords = industry_config.get("domains", [])
        keywords.extend(domain_keywords)
        
        return list(set(keywords))  # å»é‡
    
    def _determine_evidence_type(self, result: Dict[str, Any], source: str) -> EvidenceType:
        """ç¡®å®šè¯æ®ç±»å‹"""
        title = result.get("title", "").lower()
        abstract = result.get("summary", result.get("abstract", "")).lower()
        text = f"{title} {abstract}"
        
        if "paper" in text or "journal" in text or "conference" in text:
            return EvidenceType.ACADEMIC_PAPER
        elif "report" in text or "study" in text:
            return EvidenceType.INDUSTRY_REPORT
        elif "standard" in text or "specification" in text:
            return EvidenceType.STANDARD_DOCUMENT
        elif "case study" in text or "implementation" in text:
            return EvidenceType.CASE_STUDY
        elif "documentation" in text or "guide" in text:
            return EvidenceType.TECHNICAL_DOCUMENTATION
        elif "benchmark" in text or "performance" in text:
            return EvidenceType.BENCHMARK_RESULT
        elif "certification" in text or "compliance" in text:
            return EvidenceType.CERTIFICATION
        else:
            return EvidenceType.WHITE_PAPER
    
    def _validate_evidence(self, evidence: EvidenceEntry) -> bool:
        """éªŒè¯è¯æ®è´¨é‡"""
        # æ£€æŸ¥æœ€å°ç›¸å…³æ€§åˆ†æ•°
        min_score = self.config["quality_thresholds"]["min_relevance_score"]
        if evidence.relevance_score < min_score:
            return False
        
        # æ£€æŸ¥æ‘˜è¦é•¿åº¦
        min_abstract_length = self.config["quality_thresholds"]["min_abstract_length"]
        if len(evidence.abstract) < min_abstract_length:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„URLæˆ–DOI
        if not evidence.url and not evidence.doi:
            return False
        
        return True
    
    def enhance_industry_capabilities(self) -> Dict[str, IndustryCapability]:
        """å¢å¼ºè¡Œä¸šèƒ½åŠ›è¯æ®"""
        print("å¼€å§‹å¢å¼ºè¡Œä¸šèƒ½åŠ›è¯æ®...")
        
        for industry, industry_config in self.config["industries"].items():
            print(f"\nå¤„ç†è¡Œä¸š: {industry}")
            
            domains = industry_config.get("domains", [])
            required_evidence_per_domain = industry_config.get("required_evidence_per_domain", 3)
            
            for domain in domains:
                print(f"  å¤„ç†é¢†åŸŸ: {domain}")
                
                # æ„å»ºæœç´¢æŸ¥è¯¢
                query = f"{industry} {domain} formal verification"
                
                # æœç´¢è¯æ®
                evidence_entries = self.search_evidence(query, industry, domain)
                
                # åˆ›å»ºæˆ–æ›´æ–°è¡Œä¸šèƒ½åŠ›
                capability_id = f"{industry}_{domain}"
                
                if capability_id not in self.industry_capabilities:
                    capability = IndustryCapability(
                        capability_id=capability_id,
                        industry=industry,
                        domain=domain,
                        capability_name=f"{industry.title()} {domain.title()}",
                        description=f"Formal verification capabilities for {domain} in {industry}",
                        required_evidence_count=required_evidence_per_domain,
                        current_evidence_count=0,
                        evidence_entries=[],
                        coverage_score=0.0
                    )
                    self.industry_capabilities[capability_id] = capability
                
                capability = self.industry_capabilities[capability_id]
                
                # æ·»åŠ è¯æ®æ¡ç›®
                for evidence in evidence_entries:
                    if evidence.evidence_id not in capability.evidence_entries:
                        capability.evidence_entries.append(evidence.evidence_id)
                        capability.current_evidence_count += 1
                
                # è®¡ç®—è¦†ç›–åˆ†æ•°
                capability.coverage_score = min(1.0, capability.current_evidence_count / capability.required_evidence_count)
                
                print(f"    æ‰¾åˆ° {len(evidence_entries)} ä¸ªè¯æ®æ¡ç›®")
                print(f"    è¦†ç›–åˆ†æ•°: {capability.coverage_score:.2%}")
        
        return self.industry_capabilities
    
    def generate_evidence_report(self) -> str:
        """ç”Ÿæˆè¯æ®æŠ¥å‘Š"""
        report = f"""
# è¡Œä¸šè¯æ®å®è¯åŒ–æŠ¥å‘Š

## æ¦‚è§ˆ
- æ€»è¯æ®æ¡ç›®æ•°: {len(self.evidence_entries)}
- è¡Œä¸šèƒ½åŠ›æ•°: {len(self.industry_capabilities)}
- å¹³å‡è¦†ç›–åˆ†æ•°: {sum(c.coverage_score for c in self.industry_capabilities.values()) / len(self.industry_capabilities):.2%}

## è¡Œä¸šèƒ½åŠ›è¯¦æƒ…

"""
        
        for capability in self.industry_capabilities.values():
            report += f"""
### {capability.capability_name}
- **è¡Œä¸š**: {capability.industry}
- **é¢†åŸŸ**: {capability.domain}
- **å½“å‰è¯æ®æ•°**: {capability.current_evidence_count}/{capability.required_evidence_count}
- **è¦†ç›–åˆ†æ•°**: {capability.coverage_score:.2%}
- **çŠ¶æ€**: {'âœ… å®Œæˆ' if capability.coverage_score >= 1.0 else 'ğŸ”„ è¿›è¡Œä¸­' if capability.coverage_score >= 0.5 else 'âŒ ä¸è¶³'}

"""
        
        report += """
## è¯æ®æ¡ç›®è¯¦æƒ…

"""
        
        for evidence in self.evidence_entries.values():
            report += f"""
### {evidence.title}
- **ç±»å‹**: {evidence.evidence_type.value}
- **æ¥æº**: {evidence.source.value}
- **ç›¸å…³æ€§åˆ†æ•°**: {evidence.relevance_score:.2f}
- **å‘å¸ƒæ—¥æœŸ**: {evidence.publication_date.strftime('%Y-%m-%d')}
- **DOI**: {evidence.doi or 'N/A'}
- **URL**: {evidence.url or 'N/A'}
- **å…³é”®è¯**: {', '.join(evidence.keywords)}

"""
        
        return report
    
    def save_evidence_data(self, filepath: str = "evidence-data.json"):
        """ä¿å­˜è¯æ®æ•°æ®"""
        data = {
            "evidence_entries": {eid: asdict(entry) for eid, entry in self.evidence_entries.items()},
            "industry_capabilities": {cid: asdict(cap) for cid, cap in self.industry_capabilities.items()},
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"è¯æ®æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè¯æ®å¢å¼ºç³»ç»Ÿ"""
    enhancer = EvidenceEnhancer()
    
    print("=== è¡Œä¸šè¯æ®å®è¯åŒ–å¢å¼ºç³»ç»Ÿæ¼”ç¤º ===")
    
    # å¢å¼ºè¡Œä¸šèƒ½åŠ›è¯æ®
    capabilities = enhancer.enhance_industry_capabilities()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = enhancer.generate_evidence_report()
    print(report)
    
    # ä¿å­˜æ•°æ®
    enhancer.save_evidence_data()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
    print(f"æ€»è¯æ®æ¡ç›®æ•°: {len(enhancer.evidence_entries)}")
    print(f"è¡Œä¸šèƒ½åŠ›æ•°: {len(enhancer.industry_capabilities)}")
    
    completed_capabilities = sum(1 for c in capabilities.values() if c.coverage_score >= 1.0)
    print(f"å®Œæˆçš„èƒ½åŠ›æ•°: {completed_capabilities}/{len(capabilities)}")
    
    avg_coverage = sum(c.coverage_score for c in capabilities.values()) / len(capabilities)
    print(f"å¹³å‡è¦†ç›–åˆ†æ•°: {avg_coverage:.2%}")


if __name__ == "__main__":
    main()
