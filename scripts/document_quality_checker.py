#!/usr/bin/env python3
"""
æ–‡æ¡£è´¨é‡æ£€æŸ¥å™¨ - æ£€æŸ¥æ–‡æ¡£çš„è´¨é‡æŒ‡æ ‡
"""

import os
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import argparse
import statistics
from datetime import datetime

class DocumentQualityChecker:
    def __init__(self, root_dir: str = "docs"):
        self.root_dir = Path(root_dir)
        self.quality_metrics = {}
        
    def find_markdown_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶"""
        markdown_files = []
        for file_path in self.root_dir.rglob("*.md"):
            if file_path.is_file():
                markdown_files.append(file_path)
        return markdown_files
    
    def check_document_structure(self, file_path: Path) -> Dict:
        """æ£€æŸ¥æ–‡æ¡£ç»“æ„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {'error': str(e)}
        
        structure_metrics = {
            'has_title': bool(re.search(r'^#\s+.+', content, re.MULTILINE)),
            'has_toc': '## ç›®å½•' in content or '## Table of Contents' in content,
            'has_overview': '## æ¦‚è¿°' in content or '## Overview' in content,
            'has_conclusion': '## æ€»ç»“' in content or '## ç»“è®º' in content or '## Conclusion' in content,
            'has_references': '## å‚è€ƒæ–‡çŒ®' in content or '## References' in content,
            'title_count': len(re.findall(r'^#+\s+', content, re.MULTILINE)),
            'section_count': len(re.findall(r'^##\s+', content, re.MULTILINE)),
            'subsection_count': len(re.findall(r'^###\s+', content, re.MULTILINE))
        }
        
        return structure_metrics
    
    def check_content_quality(self, file_path: Path) -> Dict:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {'error': str(e)}
        
        # åŸºæœ¬ç»Ÿè®¡
        lines = content.split('\n')
        words = content.split()
        
        content_metrics = {
            'line_count': len(lines),
            'word_count': len(words),
            'char_count': len(content),
            'non_empty_lines': len([line for line in lines if line.strip()]),
            'code_blocks': len(re.findall(r'```', content)) // 2,
            'links': len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)),
            'images': len(re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)),
            'tables': len(re.findall(r'\|.*\|', content)),
            'lists': len(re.findall(r'^[\s]*[-*+]\s+', content, re.MULTILINE))
        }
        
        # å†…å®¹è´¨é‡æŒ‡æ ‡
        content_metrics.update({
            'avg_words_per_line': content_metrics['word_count'] / max(content_metrics['line_count'], 1),
            'code_block_ratio': content_metrics['code_blocks'] / max(content_metrics['line_count'], 1),
            'link_density': content_metrics['links'] / max(content_metrics['word_count'], 1),
            'structure_ratio': (content_metrics['tables'] + content_metrics['lists']) / max(content_metrics['line_count'], 1)
        })
        
        return content_metrics
    
    def check_format_consistency(self, file_path: Path) -> Dict:
        """æ£€æŸ¥æ ¼å¼ä¸€è‡´æ€§"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {'error': str(e)}
        
        format_metrics = {
            'consistent_heading_style': self._check_heading_consistency(content),
            'consistent_list_style': self._check_list_consistency(content),
            'consistent_code_style': self._check_code_consistency(content),
            'consistent_link_style': self._check_link_consistency(content),
            'has_proper_spacing': self._check_spacing(content),
            'has_proper_indentation': self._check_indentation(content)
        }
        
        return format_metrics
    
    def _check_heading_consistency(self, content: str) -> bool:
        """æ£€æŸ¥æ ‡é¢˜ä¸€è‡´æ€§"""
        headings = re.findall(r'^(#+)\s+(.+)', content, re.MULTILINE)
        if not headings:
            return True
        
        # æ£€æŸ¥æ ‡é¢˜å±‚çº§æ˜¯å¦è¿ç»­
        levels = [len(h[0]) for h in headings]
        for i in range(1, len(levels)):
            if levels[i] > levels[i-1] + 1:
                return False
        
        return True
    
    def _check_list_consistency(self, content: str) -> bool:
        """æ£€æŸ¥åˆ—è¡¨ä¸€è‡´æ€§"""
        list_items = re.findall(r'^[\s]*([-*+])\s+', content, re.MULTILINE)
        if not list_items:
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€çš„åˆ—è¡¨æ ‡è®°
        unique_markers = set(list_items)
        return len(unique_markers) == 1
    
    def _check_code_consistency(self, content: str) -> bool:
        """æ£€æŸ¥ä»£ç å—ä¸€è‡´æ€§"""
        code_blocks = re.findall(r'```(\w+)?', content)
        if not code_blocks:
            return True
        
        # æ£€æŸ¥ä»£ç å—æ˜¯å¦æœ‰è¯­è¨€æ ‡è¯†
        has_language = any(block.strip() for block in code_blocks)
        return has_language
    
    def _check_link_consistency(self, content: str) -> bool:
        """æ£€æŸ¥é“¾æ¥ä¸€è‡´æ€§"""
        links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
        if not links:
            return True
        
        # æ£€æŸ¥é“¾æ¥æ ¼å¼æ˜¯å¦ä¸€è‡´
        for link_text, link_url in links:
            if not link_text.strip() or not link_url.strip():
                return False
        
        return True
    
    def _check_spacing(self, content: str) -> bool:
        """æ£€æŸ¥é—´è·"""
        lines = content.split('\n')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šçš„ç©ºè¡Œ
        empty_line_count = 0
        for line in lines:
            if not line.strip():
                empty_line_count += 1
            else:
                empty_line_count = 0
            
            if empty_line_count > 2:
                return False
        
        return True
    
    def _check_indentation(self, content: str) -> bool:
        """æ£€æŸ¥ç¼©è¿›"""
        lines = content.split('\n')
        
        for line in lines:
            if line.strip():
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä¸€è‡´çš„ç¼©è¿›ï¼ˆç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ï¼‰
                leading_spaces = len(line) - len(line.lstrip(' '))
                leading_tabs = len(line) - len(line.lstrip('\t'))
                
                if leading_spaces > 0 and leading_tabs > 0:
                    return False
        
        return True
    
    def check_readability(self, file_path: Path) -> Dict:
        """æ£€æŸ¥å¯è¯»æ€§"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {'error': str(e)}
        
        # ç§»é™¤ä»£ç å—å’Œé“¾æ¥
        clean_content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
        clean_content = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', clean_content)
        
        sentences = re.split(r'[.!?]+', clean_content)
        words = clean_content.split()
        
        readability_metrics = {
            'sentence_count': len([s for s in sentences if s.strip()]),
            'avg_words_per_sentence': len(words) / max(len([s for s in sentences if s.strip()]), 1),
            'avg_chars_per_word': sum(len(word) for word in words) / max(len(words), 1),
            'long_sentences': len([s for s in sentences if len(s.split()) > 20]),
            'short_sentences': len([s for s in sentences if len(s.split()) < 5])
        }
        
        return readability_metrics
    
    def check_theoretical_completeness(self, file_path: Path) -> Dict:
        """æ£€æŸ¥ç†è®ºå®Œæ•´æ€§"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {'error': str(e)}
        
        theoretical_metrics = {
            'has_formal_definition': bool(re.search(r'å½¢å¼åŒ–å®šä¹‰|formal\s+definition', content, re.IGNORECASE)),
            'has_mathematical_notation': bool(re.search(r'[âˆ€âˆƒâˆˆâˆ‰âˆ§âˆ¨â†’â†”]|\\forall|\\exists|\\in|\\notin', content)),
            'has_axioms': bool(re.search(r'å…¬ç†|axiom|å…¬ç†åŒ–', content, re.IGNORECASE)),
            'has_proof': bool(re.search(r'è¯æ˜|proof|æ¨å¯¼', content, re.IGNORECASE)),
            'has_standard_alignment': bool(re.search(r'å›½é™…æ ‡å‡†|ISO|IEEE|æ ‡å‡†å¯¹é½', content, re.IGNORECASE)),
            'has_references': bool(re.search(r'##\s*å‚è€ƒæ–‡çŒ®|##\s*References', content, re.IGNORECASE)),
            'has_academic_courses': bool(re.search(r'å¤§å­¦è¯¾ç¨‹|è¯¾ç¨‹å¯¹æ ‡|MIT|Stanford|CMU', content, re.IGNORECASE)),
            'formal_definition_count': len(re.findall(r'å½¢å¼åŒ–å®šä¹‰|formal\s+definition', content, re.IGNORECASE)),
            'mathematical_formula_count': len(re.findall(r'```.*?```|\\[.*?\\]|\$\$.*?\$\$', content, re.DOTALL)),
            'standard_mention_count': len(re.findall(r'ISO|IEEE|æ ‡å‡†', content, re.IGNORECASE))
        }
        
        # è®¡ç®—ç†è®ºå®Œæ•´æ€§è¯„åˆ†
        theoretical_score = 0
        max_theoretical_score = 100
        
        if theoretical_metrics['has_formal_definition']:
            theoretical_score += 20
        if theoretical_metrics['has_mathematical_notation']:
            theoretical_score += 15
        if theoretical_metrics['has_axioms']:
            theoretical_score += 10
        if theoretical_metrics['has_proof']:
            theoretical_score += 15
        if theoretical_metrics['has_standard_alignment']:
            theoretical_score += 15
        if theoretical_metrics['has_references']:
            theoretical_score += 10
        if theoretical_metrics['has_academic_courses']:
            theoretical_score += 10
        
        # å…¬å¼å’Œå®šä¹‰æ•°é‡åŠ åˆ†
        if theoretical_metrics['formal_definition_count'] > 0:
            theoretical_score += min(5, theoretical_metrics['formal_definition_count'])
        if theoretical_metrics['mathematical_formula_count'] > 0:
            theoretical_score += min(5, theoretical_metrics['mathematical_formula_count'])
        
        theoretical_metrics['theoretical_completeness_score'] = min(theoretical_score, max_theoretical_score)
        
        return theoretical_metrics
    
    def check_learning_friendliness(self, file_path: Path) -> Dict:
        """æ£€æŸ¥å­¦ä¹ å‹å¥½æ€§"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {'error': str(e)}
        
        learning_metrics = {
            'has_concept_explanation': bool(re.search(r'æ¦‚å¿µå®šä¹‰|æ¦‚å¿µè¯´æ˜|concept\s+definition', content, re.IGNORECASE)),
            'has_examples': bool(re.search(r'ç¤ºä¾‹|example|æ¡ˆä¾‹|case', content, re.IGNORECASE)),
            'has_diagrams': bool(re.search(r'```mermaid|```graph|æµç¨‹å›¾|diagram', content, re.IGNORECASE)),
            'has_code_examples': len(re.findall(r'```(?:python|java|javascript|yaml|json)', content, re.IGNORECASE)),
            'has_learning_path': bool(re.search(r'å­¦ä¹ è·¯å¾„|learning\s+path|å‰ç½®çŸ¥è¯†', content, re.IGNORECASE)),
            'has_difficulty_level': bool(re.search(r'éš¾åº¦|difficulty|â­â­', content)),
            'has_exercises': bool(re.search(r'æ€è€ƒ|ç»ƒä¹ |exercise|practice', content, re.IGNORECASE)),
            'has_checklist': bool(re.search(r'æ£€æŸ¥æ¸…å•|checklist|éªŒè¯æ¸…å•', content, re.IGNORECASE)),
            'has_related_concepts': bool(re.search(r'ç›¸å…³æ¦‚å¿µ|related\s+concepts', content, re.IGNORECASE)),
            'has_cross_references': len(re.findall(r'\[.*?\]\(.*?\)', content)),
            'example_count': len(re.findall(r'ç¤ºä¾‹|example|æ¡ˆä¾‹', content, re.IGNORECASE)),
            'diagram_count': len(re.findall(r'```mermaid|```graph', content, re.IGNORECASE))
        }
        
        # è®¡ç®—å­¦ä¹ å‹å¥½æ€§è¯„åˆ†
        learning_score = 0
        max_learning_score = 100
        
        if learning_metrics['has_concept_explanation']:
            learning_score += 15
        if learning_metrics['has_examples']:
            learning_score += 15
        if learning_metrics['has_diagrams']:
            learning_score += 15
        if learning_metrics['has_code_examples'] > 0:
            learning_score += min(15, learning_metrics['has_code_examples'] * 3)
        if learning_metrics['has_learning_path']:
            learning_score += 10
        if learning_metrics['has_difficulty_level']:
            learning_score += 5
        if learning_metrics['has_exercises']:
            learning_score += 10
        if learning_metrics['has_checklist']:
            learning_score += 5
        if learning_metrics['has_related_concepts']:
            learning_score += 10
        
        # äº¤å‰å¼•ç”¨æ•°é‡åŠ åˆ†
        if learning_metrics['has_cross_references'] >= 5:
            learning_score += 5
        
        learning_metrics['learning_friendliness_score'] = min(learning_score, max_learning_score)
        
        return learning_metrics
    
    def calculate_quality_score(self, metrics: Dict) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†ï¼ˆåŸºç¡€ç‰ˆï¼‰"""
        if 'error' in metrics:
            return 0.0
        
        score = 0.0
        max_score = 100.0
        
        # ç»“æ„å®Œæ•´æ€§ (30åˆ†)
        structure_score = 0
        if metrics.get('has_title', False):
            structure_score += 5
        if metrics.get('has_toc', False):
            structure_score += 10
        if metrics.get('has_overview', False):
            structure_score += 5
        if metrics.get('has_conclusion', False):
            structure_score += 5
        if metrics.get('has_references', False):
            structure_score += 5
        
        score += structure_score * 0.3
        
        # å†…å®¹è´¨é‡ (40åˆ†)
        content_score = 0
        word_count = metrics.get('word_count', 0)
        if 500 <= word_count <= 10000:
            content_score += 10
        elif word_count > 100:
            content_score += 5
        
        if metrics.get('code_blocks', 0) > 0:
            content_score += 5
        
        if metrics.get('links', 0) >= 3:
            content_score += 10
        
        if metrics.get('tables', 0) > 0 or metrics.get('lists', 0) > 0:
            content_score += 5
        
        if metrics.get('images', 0) > 0:
            content_score += 5
        
        # å¯è¯»æ€§è¯„åˆ†
        avg_words_per_sentence = metrics.get('avg_words_per_sentence', 0)
        if 10 <= avg_words_per_sentence <= 20:
            content_score += 5
        
        score += content_score * 0.4
        
        # æ ¼å¼ä¸€è‡´æ€§ (30åˆ†)
        format_score = 0
        if metrics.get('consistent_heading_style', False):
            format_score += 10
        if metrics.get('consistent_list_style', False):
            format_score += 5
        if metrics.get('consistent_code_style', False):
            format_score += 5
        if metrics.get('consistent_link_style', False):
            format_score += 5
        if metrics.get('has_proper_spacing', False):
            format_score += 3
        if metrics.get('has_proper_indentation', False):
            format_score += 2
        
        score += format_score * 0.3
        
        return min(score, max_score)
    
    def calculate_enhanced_quality_score(self, metrics: Dict) -> float:
        """è®¡ç®—å¢å¼ºè´¨é‡è¯„åˆ†ï¼ˆåŒ…å«ç†è®ºå®Œæ•´æ€§å’Œå­¦ä¹ å‹å¥½æ€§ï¼‰"""
        if 'error' in metrics:
            return 0.0
        
        # åŸºç¡€è´¨é‡è¯„åˆ† (60%)
        base_score = self.calculate_quality_score(metrics)
        
        # ç†è®ºå®Œæ•´æ€§è¯„åˆ† (20%)
        theoretical_score = metrics.get('theoretical_completeness_score', 0)
        
        # å­¦ä¹ å‹å¥½æ€§è¯„åˆ† (20%)
        learning_score = metrics.get('learning_friendliness_score', 0)
        
        # ç»¼åˆè¯„åˆ†
        enhanced_score = (
            base_score * 0.6 +
            theoretical_score * 0.2 +
            learning_score * 0.2
        )
        
        return min(enhanced_score, 100.0)
    
    def check_document_quality(self, file_path: Path) -> Dict:
        """æ£€æŸ¥å•ä¸ªæ–‡æ¡£çš„è´¨é‡"""
        print(f"ğŸ“„ æ£€æŸ¥æ–‡æ¡£: {file_path.name}")
        
        # æ£€æŸ¥å„ä¸ªç»´åº¦
        structure_metrics = self.check_document_structure(file_path)
        content_metrics = self.check_content_quality(file_path)
        format_metrics = self.check_format_consistency(file_path)
        readability_metrics = self.check_readability(file_path)
        theoretical_metrics = self.check_theoretical_completeness(file_path)
        learning_metrics = self.check_learning_friendliness(file_path)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        all_metrics = {
            **structure_metrics,
            **content_metrics,
            **format_metrics,
            **readability_metrics,
            **theoretical_metrics,
            **learning_metrics
        }
        
        # è®¡ç®—è´¨é‡è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰
        quality_score = self.calculate_enhanced_quality_score(all_metrics)
        all_metrics['quality_score'] = quality_score
        
        return all_metrics
    
    def check_all_documents(self) -> Dict:
        """æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£çš„è´¨é‡"""
        print("ğŸ” å¼€å§‹æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£çš„è´¨é‡...")
        
        markdown_files = self.find_markdown_files()
        print(f"ğŸ“ æ‰¾åˆ° {len(markdown_files)} ä¸ªMarkdownæ–‡ä»¶")
        
        all_results = {
            'total_files': len(markdown_files),
            'file_results': [],
            'overall_metrics': {},
            'quality_distribution': {}
        }
        
        quality_scores = []
        
        # é€ä¸ªæ£€æŸ¥æ–‡æ¡£
        for i, file_path in enumerate(markdown_files, 1):
            print(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(markdown_files)}: {file_path.name}")
            
            try:
                result = self.check_document_quality(file_path)
                result['file_path'] = str(file_path)
                all_results['file_results'].append(result)
                
                if 'quality_score' in result:
                    quality_scores.append(result['quality_score'])
                
            except Exception as e:
                print(f"âŒ å¤„ç† {file_path} æ—¶å‡ºé”™: {e}")
                all_results['file_results'].append({
                    'file_path': str(file_path),
                    'error': str(e),
                    'quality_score': 0.0
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        if quality_scores:
            all_results['overall_metrics'] = {
                'avg_quality_score': statistics.mean(quality_scores),
                'median_quality_score': statistics.median(quality_scores),
                'min_quality_score': min(quality_scores),
                'max_quality_score': max(quality_scores),
                'std_quality_score': statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0
            }
            
            # è´¨é‡åˆ†å¸ƒ
            excellent = len([s for s in quality_scores if s >= 90])
            good = len([s for s in quality_scores if 80 <= s < 90])
            fair = len([s for s in quality_scores if 70 <= s < 80])
            poor = len([s for s in quality_scores if s < 70])
            
            all_results['quality_distribution'] = {
                'excellent': excellent,
                'good': good,
                'fair': fair,
                'poor': poor
            }
        
        return all_results
    
    def generate_report(self, results: Dict) -> str:
        """ç”Ÿæˆè´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
        report = []
        report.append("# ğŸ“Š æ–‡æ¡£è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
        report.append("")
        report.append(f"**æ£€æŸ¥æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        report.append("## ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
        report.append("")
        report.append(f"- **æ€»æ–‡ä»¶æ•°**: {results['total_files']}")
        
        if 'overall_metrics' in results and results['overall_metrics']:
            metrics = results['overall_metrics']
            report.append(f"- **å¹³å‡è´¨é‡è¯„åˆ†**: {metrics['avg_quality_score']:.1f}")
            report.append(f"- **ä¸­ä½æ•°è´¨é‡è¯„åˆ†**: {metrics['median_quality_score']:.1f}")
            report.append(f"- **æœ€ä½è´¨é‡è¯„åˆ†**: {metrics['min_quality_score']:.1f}")
            report.append(f"- **æœ€é«˜è´¨é‡è¯„åˆ†**: {metrics['max_quality_score']:.1f}")
            report.append(f"- **è´¨é‡è¯„åˆ†æ ‡å‡†å·®**: {metrics['std_quality_score']:.1f}")
        
        # ç†è®ºå®Œæ•´æ€§å’Œå­¦ä¹ å‹å¥½æ€§ç»Ÿè®¡
        file_results = [r for r in results['file_results'] if 'theoretical_completeness_score' in r]
        if file_results:
            theoretical_scores = [r.get('theoretical_completeness_score', 0) for r in file_results]
            learning_scores = [r.get('learning_friendliness_score', 0) for r in file_results]
            
            report.append("")
            report.append("### ç†è®ºå®Œæ•´æ€§å’Œå­¦ä¹ å‹å¥½æ€§ç»Ÿè®¡")
            report.append("")
            report.append(f"- **å¹³å‡ç†è®ºå®Œæ•´æ€§è¯„åˆ†**: {statistics.mean(theoretical_scores):.1f}")
            report.append(f"- **å¹³å‡å­¦ä¹ å‹å¥½æ€§è¯„åˆ†**: {statistics.mean(learning_scores):.1f}")
        
        if 'quality_distribution' in results and results['quality_distribution']:
            dist = results['quality_distribution']
            total = sum(dist.values())
            report.append("")
            report.append("### è´¨é‡åˆ†å¸ƒ")
            report.append("")
            report.append(f"- **ä¼˜ç§€ (90-100åˆ†)**: {dist['excellent']} ä¸ªæ–‡ä»¶ ({dist['excellent']/total*100:.1f}%)")
            report.append(f"- **è‰¯å¥½ (80-89åˆ†)**: {dist['good']} ä¸ªæ–‡ä»¶ ({dist['good']/total*100:.1f}%)")
            report.append(f"- **ä¸€èˆ¬ (70-79åˆ†)**: {dist['fair']} ä¸ªæ–‡ä»¶ ({dist['fair']/total*100:.1f}%)")
            report.append(f"- **è¾ƒå·® (<70åˆ†)**: {dist['poor']} ä¸ªæ–‡ä»¶ ({dist['poor']/total*100:.1f}%)")
        
        report.append("")
        
        # è¯¦ç»†ç»“æœ
        report.append("## ğŸ“‹ è¯¦ç»†ç»“æœ")
        report.append("")
        
        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        file_results = [r for r in results['file_results'] if 'quality_score' in r]
        file_results.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        
        # æ˜¾ç¤ºå‰10ä¸ªå’Œå10ä¸ª
        report.append("### ğŸ† è´¨é‡è¯„åˆ†æœ€é«˜çš„10ä¸ªæ–‡æ¡£")
        report.append("")
        for i, result in enumerate(file_results[:10], 1):
            score = result.get('quality_score', 0)
            file_name = Path(result['file_path']).name
            report.append(f"{i}. **{file_name}** - {score:.1f}åˆ†")
        report.append("")
        
        report.append("### âš ï¸ è´¨é‡è¯„åˆ†æœ€ä½çš„10ä¸ªæ–‡æ¡£")
        report.append("")
        for i, result in enumerate(file_results[-10:], 1):
            score = result.get('quality_score', 0)
            file_name = Path(result['file_path']).name
            report.append(f"{len(file_results)-10+i}. **{file_name}** - {score:.1f}åˆ†")
        report.append("")
        
        # éœ€è¦æ”¹è¿›çš„æ–‡æ¡£
        poor_quality_files = [r for r in file_results if r.get('quality_score', 0) < 70]
        if poor_quality_files:
            report.append("## ğŸš¨ éœ€è¦æ”¹è¿›çš„æ–‡æ¡£")
            report.append("")
            for result in poor_quality_files:
                score = result.get('quality_score', 0)
                file_name = Path(result['file_path']).name
                report.append(f"- **{file_name}** - {score:.1f}åˆ†")
                
                # æä¾›æ”¹è¿›å»ºè®®
                suggestions = self.get_improvement_suggestions(result)
                if suggestions:
                    report.append("  - æ”¹è¿›å»ºè®®:")
                    for suggestion in suggestions:
                        report.append(f"    - {suggestion}")
                report.append("")
        
        return "\n".join(report)
    
    def get_improvement_suggestions(self, result: Dict) -> List[str]:
        """è·å–æ”¹è¿›å»ºè®®"""
        suggestions = []
        
        if not result.get('has_title', False):
            suggestions.append("æ·»åŠ æ–‡æ¡£æ ‡é¢˜")
        
        if not result.get('has_toc', False):
            suggestions.append("æ·»åŠ ç›®å½•ç»“æ„")
        
        if not result.get('has_overview', False):
            suggestions.append("æ·»åŠ æ¦‚è¿°éƒ¨åˆ†")
        
        if not result.get('has_references', False):
            suggestions.append("æ·»åŠ å‚è€ƒæ–‡çŒ®")
        
        word_count = result.get('word_count', 0)
        if word_count < 500:
            suggestions.append("å¢åŠ æ–‡æ¡£å†…å®¹ï¼Œå»ºè®®è‡³å°‘500å­—")
        elif word_count > 10000:
            suggestions.append("æ–‡æ¡£è¿‡é•¿ï¼Œå»ºè®®æ‹†åˆ†ä¸ºå¤šä¸ªæ–‡æ¡£")
        
        if result.get('links', 0) < 3:
            suggestions.append("å¢åŠ å†…éƒ¨é“¾æ¥å’Œå¤–éƒ¨å¼•ç”¨")
        
        if result.get('code_blocks', 0) == 0:
            suggestions.append("æ·»åŠ ä»£ç ç¤ºä¾‹")
        
        if not result.get('consistent_heading_style', False):
            suggestions.append("ç»Ÿä¸€æ ‡é¢˜æ ¼å¼")
        
        if not result.get('consistent_list_style', False):
            suggestions.append("ç»Ÿä¸€åˆ—è¡¨æ ¼å¼")
        
        # ç†è®ºå®Œæ•´æ€§å»ºè®®
        if not result.get('has_formal_definition', False):
            suggestions.append("æ·»åŠ å½¢å¼åŒ–å®šä¹‰")
        
        if not result.get('has_mathematical_notation', False):
            suggestions.append("æ·»åŠ æ•°å­¦ç¬¦å·å’Œå…¬å¼")
        
        if not result.get('has_standard_alignment', False):
            suggestions.append("æ·»åŠ å›½é™…æ ‡å‡†å¯¹é½å†…å®¹")
        
        if result.get('theoretical_completeness_score', 0) < 50:
            suggestions.append("æå‡ç†è®ºå®Œæ•´æ€§ï¼Œæ·»åŠ å½¢å¼åŒ–å®šä¹‰å’Œæ ‡å‡†å¯¹é½")
        
        # å­¦ä¹ å‹å¥½æ€§å»ºè®®
        if not result.get('has_concept_explanation', False):
            suggestions.append("æ·»åŠ æ¦‚å¿µè§£é‡Š")
        
        if not result.get('has_examples', False):
            suggestions.append("æ·»åŠ ç¤ºä¾‹å’Œæ¡ˆä¾‹")
        
        if not result.get('has_diagrams', False):
            suggestions.append("æ·»åŠ æµç¨‹å›¾æˆ–ç¤ºæ„å›¾")
        
        if result.get('has_code_examples', 0) == 0:
            suggestions.append("æ·»åŠ ä»£ç ç¤ºä¾‹")
        
        if not result.get('has_learning_path', False):
            suggestions.append("æ·»åŠ å­¦ä¹ è·¯å¾„è¯´æ˜")
        
        if not result.get('has_related_concepts', False):
            suggestions.append("æ·»åŠ ç›¸å…³æ¦‚å¿µé“¾æ¥")
        
        if result.get('learning_friendliness_score', 0) < 50:
            suggestions.append("æå‡å­¦ä¹ å‹å¥½æ€§ï¼Œæ·»åŠ ç¤ºä¾‹ã€å›¾è¡¨å’Œå­¦ä¹ è·¯å¾„")
        
        return suggestions
    
    def save_report(self, results: Dict, output_file: str = "document_quality_report.md"):
        """ä¿å­˜è´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
        report = self.generate_report(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ è´¨é‡æ£€æŸ¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥æ–‡æ¡£è´¨é‡')
    parser.add_argument('--root', default='docs', help='æ–‡æ¡£æ ¹ç›®å½• (é»˜è®¤: docs)')
    parser.add_argument('--output', default='document_quality_report.md', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    
    args = parser.parse_args()
    
    checker = DocumentQualityChecker(args.root)
    results = checker.check_all_documents()
    checker.save_report(results, args.output)
    
    # æ‰“å°ç®€è¦ç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š è´¨é‡æ£€æŸ¥å®Œæˆç»Ÿè®¡:")
    print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
    
    if 'overall_metrics' in results and results['overall_metrics']:
        metrics = results['overall_metrics']
        print(f"å¹³å‡è´¨é‡è¯„åˆ†: {metrics['avg_quality_score']:.1f}")
        print(f"è´¨é‡è¯„åˆ†èŒƒå›´: {metrics['min_quality_score']:.1f} - {metrics['max_quality_score']:.1f}")
    
    if 'quality_distribution' in results and results['quality_distribution']:
        dist = results['quality_distribution']
        print(f"ä¼˜ç§€æ–‡æ¡£: {dist['excellent']} ä¸ª")
        print(f"éœ€è¦æ”¹è¿›: {dist['poor']} ä¸ª")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
